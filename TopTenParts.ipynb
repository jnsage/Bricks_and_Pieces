{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopTenParts | Highest Quantity Parts in a LEGO Set\n",
    "\n",
    "#### This notebook will find the Star Wars LEGO set with the most pieces, and then find the 10 most common parts within that set. It uses LEGO set, part, and theme information from [Rebrickable](https://rebrickable.com/downloads/). The data is snapshots of multiple tables from the Rebrickable database downloaded in July 2022. Rebrickable updates their files daily. This notebook will read 5 .csv files, merge data, clean data, and create \"top_ten_parts.csv\" in the \"CSVs\" folder in this repo. This .csv file is used in a [Tableau Dashboard](https://public.tableau.com/app/profile/jared.sage/viz/BricksandPieces/BricksandPieces).\n",
    "\n",
    "#### The results of this notebook will be an exact answer to the 10 most common parts.\n",
    "\n",
    "#### Start by importing the necessary packages to run the notebook. Pandas will be used for importing, cleaning, and merging data. Requests will be used for API calls. PythonScripts is a folder of scripts made for this project. data_clean is for functions used across the 2 or more notebooks in this repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import PythonScripts.data_clean as dc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to create several DataFrames based on the Rebrickable data. The database diagram on the [Rebrickable downloads](https://rebrickable.com/downloads/) page shows the relationship between the tables and will frame how we merge the data. We are going to create a DataFrame for:\n",
    "\n",
    "- Inventory IDs/Set Numbers\n",
    "- Inventory Parts - all the parts and part quantities within each inventory ID\n",
    "- Sets - all the details for a set number\n",
    "- Parts - all the details about a part, such as color\n",
    "- Colors - all the details about a specific color\n",
    "\n",
    "#### We are only going to read in the columns needed to reduce the amount of data we'll have to parse through. Output is the shape and first 3 rows of each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs off local drive and convert them each to a DataFrame\n",
    "\n",
    "inv_path = dc.csv_path('inventories.csv')\n",
    "inv_df = pd.read_csv(inv_path, usecols=['id','set_num'])\n",
    "\n",
    "inv_parts_path = dc.csv_path('inventory_parts.csv')\n",
    "inv_parts_df = pd.read_csv(inv_parts_path, usecols=['inventory_id', 'part_num', 'color_id', 'quantity'])\n",
    "\n",
    "sets_path = dc.csv_path('sets.csv')\n",
    "set_df = pd.read_csv(sets_path, usecols=['set_num', 'name', 'year', 'theme_id', 'num_parts'])\n",
    "\n",
    "parts_path = dc.csv_path('parts.csv')\n",
    "parts_df = pd.read_csv(parts_path, usecols=['part_num', 'name'])\n",
    "\n",
    "colors_path = dc.csv_path('colors.csv')\n",
    "colors_df = pd.read_csv(colors_path, usecols=['id', 'name', 'rgb'])\n",
    "\n",
    "print(f'inv_df shape: {inv_df.shape}')\n",
    "print(inv_df.head(n=3))\n",
    "print(f'\\ninv_parts_df shape: {inv_parts_df.shape}')\n",
    "print(inv_parts_df.head(n=3))\n",
    "print(f'\\nset_df shape: {set_df.shape}')\n",
    "print(set_df.head(n=3))\n",
    "print(f'\\nparts_df shape: {parts_df.shape}')\n",
    "print(parts_df.head(n=3))\n",
    "print(f'\\ncolors_df shape: {colors_df.shape}')\n",
    "print(colors_df.head(n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will perform several merges to create one giant DataFrame. To make merges possible, we rename column labels across DataFrames so that Series with the same data point/value have the same column name. Output is the starting and new column labels for the inv_df, parts_df, and color_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in inv_df and parts_df for easier joining\n",
    "print('Original column labels:')\n",
    "print(f'inv_df: {inv_df.columns}')\n",
    "print(f'parts_df: {parts_df.columns}')\n",
    "print(f'colors_df: {colors_df.columns}')\n",
    "\n",
    "inv_rename_dict = {'id' : 'inventory_id',\n",
    "                   'set_num' : 'set_num'}\n",
    "inv_df.rename(columns=inv_rename_dict, inplace=True)\n",
    "\n",
    "parts_rename_dict = {'part_num' : 'part_num',\n",
    "                     'name' : 'part_name'}\n",
    "parts_df.rename(columns=parts_rename_dict, inplace=True)\n",
    "\n",
    "colors_rename_dict = {'id' : 'color_id',\n",
    "                     'name' : 'color_name'}\n",
    "colors_df.rename(columns=colors_rename_dict, inplace=True)\n",
    "\n",
    "print('\\nRenamed column labels:')\n",
    "print(f'inv_df: {inv_df.columns}')\n",
    "print(f'parts_df: {parts_df.columns}')\n",
    "print(f'colors_df: {colors_df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we merge. Starting point is the Inventory ID DataFrame. We then do 4 joins to add on data in the following order:\n",
    "1. Inventory Parts - part_numbers, color, and quantities for each part\n",
    "2. Sets - set names, themes, release year, and number of total parts in the set\n",
    "3. Parts - the name of each part\n",
    "4. Color - color name and hex color code (rgb column)\n",
    "\n",
    "#### Output shows the increasing number of columns added on to the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrames into one larger DataFrame with all data points\n",
    "print(f'Original inv_df shape: {inv_df.shape}')\n",
    "\n",
    "all_merged_df = inv_df.merge(inv_parts_df, how='inner', left_on='inventory_id', right_on='inventory_id')\n",
    "print(f'Post-first merge shape: {all_merged_df.shape}')\n",
    "      \n",
    "all_merged_df = all_merged_df.merge(set_df, how='left', left_on='set_num', right_on='set_num')\n",
    "print(f'Post-second merge shape: {all_merged_df.shape}')\n",
    "\n",
    "all_merged_df = all_merged_df.merge(parts_df, how='inner', left_on='part_num', right_on='part_num')\n",
    "print(f'Post-third all_merged_df shape: {all_merged_df.shape}')\n",
    "\n",
    "all_merged_df = all_merged_df.merge(colors_df, how='inner', left_on='color_id', right_on='color_id')\n",
    "print(f'current all_merged_df shape: {all_merged_df.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our DataFrame currently has information for all LEGO sets. But we want to focus on Star Wars-themed sets. We reduce the DataFrame to sets that have one of the theme_ids relating to Star Wars. Output shows the theme IDs we wanted and the unique theme IDs after removing rows. They should be equal to verify we only have what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non-Star Wars themes from the Dataframe\n",
    "sw_theme_ids = [18, 158, 171, 209, 261]\n",
    "print(f'Star Wars Theme IDs: {sw_theme_ids}')\n",
    "all_merged_df = all_merged_df[all_merged_df['theme_id'].isin(sw_theme_ids)]\n",
    "unique_themes = all_merged_df['theme_id'].unique()\n",
    "\n",
    "print(f'Unique values in theme_id column in DataFrame: {unique_themes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to figure out what the Star Wars set with the most pieces is. We group quantities of parts by set number and report the sums. Then we can find the set number with the highest sum. Output is the set number with the most parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the set number with the most pieces\n",
    "piece_count = all_merged_df.groupby(['set_num'])['quantity'].sum()\n",
    "max_count = piece_count.idxmax()\n",
    "print(f'Set with highest count: {max_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we make a DataFrame containing rows where the set number equals the set number with the most parts. Output shows the shape of the DataFrame, now containing only the 730 parts, and the first 3 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows from the large DataFrame except the set with most pieces\n",
    "print(f'Current all_merged_df shape: {all_merged_df.shape}')\n",
    "\n",
    "mask = all_merged_df[all_merged_df['set_num'] != max_count].index\n",
    "all_merged_df.drop(mask, inplace=True)\n",
    "\n",
    "print(f'Final all_merged_df shape: {all_merged_df.shape}')\n",
    "print(f'\\n{all_merged_df.head(n=3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the above rows we can see that the year, theme_id, and num_parts columns are currently floats. To assist with any calculations in Tableau and a cleaner look, convert those columns to int values. Output shows new dtypes for values in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year, theme_id, and num_parts columns to integers\n",
    "all_merged_df['year'] = all_merged_df['year'].astype(pd.Int64Dtype())\n",
    "all_merged_df['theme_id'] = all_merged_df['theme_id'].astype(pd.Int64Dtype())\n",
    "all_merged_df['num_parts'] = all_merged_df['num_parts'].astype(pd.Int64Dtype())\n",
    "print('year dtype: ',type(all_merged_df['year'].iloc[1]))\n",
    "print('theme_id dtype: ',type(all_merged_df['theme_id'].iloc[1]))\n",
    "print('num_parts dtype: ',type(all_merged_df['num_parts'].iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the DataFrame has information for all 730 part numbers for the set in question. We only want to know info about the top 10 parts. We can find the 10 largest values in the quantity column and make a new DataFrame containing all information for rows that have those 10 largest values. Output is the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new DataFrame with just the top 10 parts with highest quantity.\n",
    "top_ten_parts = all_merged_df['quantity'].nlargest(n=10, keep='first')\n",
    "top_ten_df = all_merged_df[all_merged_df['quantity'].isin(top_ten_parts)]\n",
    "top_ten_df.reset_index(drop=True, inplace=True)\n",
    "top_ten_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning and merging is complete. We save that data as a .csv to use in Tableau. A file called \"top_ten_parts.csv\" will be created in the \"CSVs\" folder of this repo if you would like to review the final output. Output is the .csv file in the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a .csv of the parts on the local drive for use in Tableau visualization\n",
    "file_path = dc.csv_path('top_ten_parts.csv')\n",
    "top_ten_df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
