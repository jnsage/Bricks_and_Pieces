{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPRatings | Popularity of Star Wars Titles compared to Brickset Ratings\n",
    "\n",
    "#### This notebook generates data to investigate a relationship between the [Rotten Tomatoes](https://rottentomatoes.com) scores for Star Wars titles and the ratings of Star Wars LEGO sets on [Brickset](https://brickset.com/). The Rotten Tomatoes scores were compiled in July 2022 into [SWMovies Google Sheet](https://docs.google.com/spreadsheets/d/1xw7y9yawF6i35BTfP9M1uUawJvwpacz01Xq4MEZszBs). The final output of this will be a .csv, ultimately used in a [Tableau Dashboard](https://public.tableau.com/app/profile/jared.sage/viz/BricksandPieces/BricksandPieces). This notebook will merge two DataFrames and generate \"tomato.csv\" in the \"CSVs\" folder in this repo.\n",
    "\n",
    "#### Prediction: LEGO set popularity will not directly correlate to the popularity of a Star Wars title. There are likely more, undetermined, factors specific to rating LEGO sets beyond popularity of a licensed title.\n",
    "\n",
    "#### Start by importing the necessary packages to run the notebook. Pandas will be used for importing, cleaning, and merging data. Requests will be used for API calls. PythonScripts is a folder of scripts made for this project. KEY_TWO will be the Brickset API key. data_clean is for functions used across the 2 or more notebooks in this repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PythonScripts.keys import KEY_TWO\n",
    "import PythonScripts.data_clean as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the parameters for reading the Star Wars title scores Google Sheet into a DataFrame. The parameters are then fed into pandas.read_csv() to create the DataFrame. Output is the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure URL for pd.read_csv and read as DataFrame\n",
    "# Full sheet URL == https://docs.google.com/spreadsheets/d/1xw7y9yawF6i35BTfP9M1uUawJvwpacz01Xq4MEZszBs/\n",
    "workbook_id = \"1xw7y9yawF6i35BTfP9M1uUawJvwpacz01Xq4MEZszBs\"\n",
    "sheet_name = \"Tomato\"\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{workbook_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "ip_df = pd.read_csv(url, parse_dates=['Release_Date'])\n",
    "ip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Release Date column looks messy. There are values with full dates and some that just start on January 1st. To clean this up, we format these values to just the year of the date. Output shows the new values of the release dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Date column to display as the year\n",
    "ip_df['Release_Date'] = ip_df['Release_Date'].dt.strftime('%Y')\n",
    "ip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to do a little more cleaning to make the merge easier down the road. We first remove the subtitles from titles with \"Episode\". We then define and call a function to remove \"Star Wars: \" from other properties leaving only the subtitles. Output shows the newly renamed values in the Title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean extra text out the Title column\n",
    "\n",
    "for item, str in ip_df['Title'].items():\n",
    "        head, sep, tail = str.partition(' â€“ ')\n",
    "        ip_df['Title'].replace(to_replace=str, value = head, inplace=True)\n",
    "\n",
    "\n",
    "# Function for cleaning a series by partition\n",
    "def part_colon(column_label: pd.Series) -> pd.Series:\n",
    "        for item, value in column_label.items():\n",
    "           if ': ' in value:\n",
    "                head, sep, tail = value.partition(': ')\n",
    "                column_label.replace(to_replace=value, value = tail, inplace=True)\n",
    "        \n",
    "# Run cleaning function on Title column\n",
    "part_colon(ip_df['Title'])\n",
    "ip_df['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are two values for Clone Wars with very different ratings. Contextually, the movie is a pilot episode for the TV show. So as a smaller part of a bigger 7-season show, we will drop the movie rating to eliminate the duplicate. This finishes the cleaning of the first DataFrame. Output shows the title Series with only one \"The Clone Wars\" in the title row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate Clone Wars, first 3 episodes of TV show theatrical release. Duplicate value and outlier\n",
    "ip_df.drop(index=9, inplace=True)\n",
    "ip_df.reset_index(drop=True, inplace=True)\n",
    "ip_df['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second DataFrame starts with an API call to Brickset. This retrieves LEGO set information for all sets under the Star Wars theme. Output shows the shape and head of the new sw_df DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call for information for sets in Star Wars theme and convert to dataframe. \n",
    "parameters = {'theme' : 'Star Wars', 'pageSize' : 900}\n",
    "sw_set_list = requests.get(f\"https://brickset.com/api/v3.asmx/getSets?apiKey={KEY_TWO}&userHash=&params={parameters}\")\n",
    "sw_data = sw_set_list.json()\n",
    "sw_df = pd.json_normalize(sw_data,'sets')\n",
    "print(f'sw_df shape: {sw_df.shape}')\n",
    "sw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The new DataFrame has 44 columns...a lot of columns. We will not need the majority of those so we will drop them out of the DataFrame. Output shows the shape of the DataFrame with reduced columns and DataFrame head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.drop_columns(sw_df)\n",
    "print(f'sw_df shape: {sw_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subthemes in sw_df are the closest match to Star Wars titles. We rename some of the subthemes to make them equivalent to the titles in ip_df. This allows for a cleaner merge. Output shows the starting values in the subtheme column and the new values after replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace certain values with values matching first data frame\n",
    "subthemes = sw_df['subtheme'].sort_values().unique()\n",
    "print(f'Subthemes: f{subthemes}')\n",
    "\n",
    "sw_df['subtheme'].replace(to_replace={'The Clone Wars' : 'Star Wars: The Clone Wars', \n",
    "                                       'The Force Awakens' : 'Episode VII', \n",
    "                                       'The Last Jedi' : 'Episode VIII', \n",
    "                                       'The Rise of Skywalker' : 'Episode IX' }, inplace=True)\n",
    "subthemes = sw_df['subtheme'].sort_values().unique()\n",
    "print(f'\\nRenamed Subthemes: f{subthemes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be safe, we want to get rid of any sets that have not been rated or rated at 0 so that it doesn't skew any of the results. Output will show the shape of the DataFrame. If the shape hasn't changed, then there are no sets that are equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop rows where there is no rating for the set.\n",
    "print('sw_df shape: ',sw_df.shape)\n",
    "\n",
    "mask_two = sw_df[sw_df['rating'] == 0].index\n",
    "sw_df.drop(mask_two, inplace=True)   \n",
    "\n",
    "print('new sw_df shape: ',sw_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also want to git rid of any LEGO sets that have a NaN value for the number of pieces in the set so that they don't skew any analysis. Output will show a shape with fewer rows if there are any sets with a NaN value for pieces. If unchanged then there were no NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows if they have a NaN value in the pieces column\n",
    "pieces_null = sw_df.isnull().values.any()\n",
    "if pieces_null == True:\n",
    "    sw_df.dropna(subset=['pieces'], inplace=True)\n",
    "                 \n",
    "print(f'sw_df shape: {sw_df.shape}')\n",
    "sw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previously shown heads of the DataFrame, you can see that the number of pieces is currently a float value. As you can't have a partial piece, we want to convert that to an integer value. Output shows new int type for a a value in the pieces column and the DataFrame head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pieces to Int64 \n",
    "sw_df['pieces'] = sw_df['pieces'].astype(pd.Int64Dtype())\n",
    "dtype = type(sw_df['pieces'].iloc[1])\n",
    "print(\"Dtype of sw_df['pieces']: \", dtype)\n",
    "sw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We run the part_colon function to remove \"Star Wars: \" from subthemes leaving only subtitles for the value. This will allow a clean merge between titles and subthemes. Output shows the new values in the subtheme column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clean via partition function on the subtheme column of the second dataframe\n",
    "part_colon(sw_df['subtheme'])\n",
    "sw_df['subtheme'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While we could merge DataFrames here, what we really want to know about LEGO sets is specific to how they relate to the subtheme column. We group set number by each subtheme and count the set numbers to determine how many sets are in each theme. That is saved as a new Series. Output is the new lego_set_count Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group subthemes by the number of sets in the the subtheme\n",
    "lego_set_count = sw_df.groupby(['subtheme'])['number'].count()\n",
    "lego_set_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next set of information we want is the average Brickset rating by subtheme. We group rating by subtheme and then calculate the average. That is saved as a new Series. Output is the rating_avg Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group subthemes by the average rating\n",
    "rating_avg = sw_df.groupby(['subtheme'])['rating'].mean().round(2)\n",
    "rating_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is to concatenate the 2 calculated Series into a new DataFrame. This is all the set information needed to then merge with our information by title. Output is the new agg_df DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame combining the set count and rating by subtheme\n",
    "agg_df = pd.concat([lego_set_count, rating_avg], axis=1)\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we perform the merge. We are going to do a left join based on the title so that we only get subtheme information for subthemes that match a title, eliminating extraneous data. Output is the new merged_df DataFrame showing all the ip_df data with new \"number\" and \"rating\" columns from agg_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrame of set #s and average rating into DataFrame of Star Wars properties\n",
    "merged_df = ip_df.merge(agg_df, how='left', left_on='Title', right_on='subtheme')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eventually we want to compare values in the Tomatometer column with Brickset ratings in Tableau. We want to make sure that we have numerical values for the Tomatometer instead of the string values that currently exist. We remove the \"%\" character, convert the resulting characters to float values, and convert them to decimal percentages. Output is the DataFrame head showing the new Tomatometer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace percentage string with a float value for percentage rating\n",
    "for index, value in merged_df['Tomatometer'].items():\n",
    "    x = value.strip('%')\n",
    "    merged_df['Tomatometer'] = merged_df['Tomatometer'].replace(value, x)\n",
    "merged_df['Tomatometer'] = merged_df['Tomatometer'].astype(float)\n",
    "merged_df['Tomatometer'] = merged_df['Tomatometer']/100\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to compare Brickset Ratings to Rotten Tomato scores. However, a problem can be seen by looking at the DataFrame. The two websites use different scales. Brickset uses a 5-point scale, while Rotten Tomatoes uses a 100-point percentage scale. To make things easier we add a new Series with Brickset ratings converted to a 100-point percentage scale. Output shows the the DataFrame head with the new Series \"Brickset % Rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new column that converts Bricket rating from 5 point scale to a percentage scale\n",
    "merged_df['Tomatometer'] = merged_df['Tomatometer'].astype(float)\n",
    "merged_df['Brickset % Rating'] =  merged_df['rating'] / 5\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning and merging is complete. We save that data as a .csv to use in Tableau. A file called \"tomato.csv\" will be created in the \"CSVs\" folder of this repo if you would like to review the final output. Output is the .csv file in the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the merged DataFrame to .csv for visualization in Tableau\n",
    "file_path = dc.csv_path('tomato.csv')\n",
    "merged_df.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
